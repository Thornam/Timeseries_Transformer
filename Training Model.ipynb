{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21334,"status":"ok","timestamp":1702925767529,"user":{"displayName":"Jacob Eriksen","userId":"07841893324515475290"},"user_tz":-60},"id":"nNkURIGjipD5","outputId":"38c1bacd-77c9-4de7-d190-3c09e6516bec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# This code will connect the Colab program to the Google Drive folder\n","# It will create a pop-up window where access has to be granted\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rrno-oYNiCnC"},"outputs":[],"source":["import torch\n","import datetime\n","import numpy as np\n","import pandas as pd\n","\n","import torch.nn as nn\n","import math\n","from torch import nn, Tensor\n","\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from datetime import timedelta\n","\n","import random\n","import sys\n","\n","# Path to the Transformer folder\n","path = \"/content/drive/My Drive/Timeseries_Transformer\" ########### This line should be changed ###########"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrYqu_yayKYZ"},"outputs":[],"source":["\n","# Checks if the program is running with a GPU\n","assert torch.cuda.is_available() == True, f\"GPU is not available, please select another run-time type in Colab that does support GPU.\"\n","\n","# Sets the device to be the GPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PZupjWbjEFl"},"outputs":[],"source":["\n","# Sets path read in code from the Transformer folder\n","sys.path.insert(0, path)\n","\n","import utils\n","import Transformer_Model as tsModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEHuLnC0iCnG"},"outputs":[],"source":["# Parameters:\n","dim_val = 512\n","n_heads = 8\n","n_decoder_layers = 4\n","n_encoder_layers = 4\n","step_size = 1 # Step size, i.e. how many time steps does the moving window move at each step\n","in_features_encoder_linear_layer = 2048\n","in_features_decoder_linear_layer = 2048\n","\n","batch_first = False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":7849,"status":"ok","timestamp":1702925788286,"user":{"displayName":"Jacob Eriksen","userId":"07841893324515475290"},"user_tz":-60},"id":"pnWJClX4iCnH","outputId":"f6ff877e-ec66-4beb-9377-9c9821b80cbc"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-b8fc46a3-ab51-4e0f-a915-140a4911d16f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Ticker</th>\n","      <th>Start</th>\n","      <th>End</th>\n","      <th>Std</th>\n","      <th>Mean</th>\n","      <th>Target</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>...</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","      <th>50</th>\n","      <th>51</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0848680D US Equity</td>\n","      <td>2011-12-26</td>\n","      <td>2012-12-24</td>\n","      <td>0.070169</td>\n","      <td>0.018523</td>\n","      <td>-0.571562</td>\n","      <td>-0.080081</td>\n","      <td>-1.143180</td>\n","      <td>0.431351</td>\n","      <td>-0.639025</td>\n","      <td>...</td>\n","      <td>-0.289046</td>\n","      <td>-0.748845</td>\n","      <td>0.395460</td>\n","      <td>-0.615254</td>\n","      <td>-0.417208</td>\n","      <td>-0.085343</td>\n","      <td>-0.012613</td>\n","      <td>-0.414256</td>\n","      <td>-0.518012</td>\n","      <td>-0.418591</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ON US Equity</td>\n","      <td>2011-12-26</td>\n","      <td>2012-12-24</td>\n","      <td>0.045480</td>\n","      <td>-0.004158</td>\n","      <td>2.034040</td>\n","      <td>0.435876</td>\n","      <td>1.908284</td>\n","      <td>-0.641805</td>\n","      <td>2.036218</td>\n","      <td>...</td>\n","      <td>0.164235</td>\n","      <td>0.451889</td>\n","      <td>1.172715</td>\n","      <td>-0.811848</td>\n","      <td>-0.887582</td>\n","      <td>1.158916</td>\n","      <td>1.444163</td>\n","      <td>0.584085</td>\n","      <td>0.763100</td>\n","      <td>-0.483041</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>OMV AV Equity</td>\n","      <td>2011-12-26</td>\n","      <td>2012-12-24</td>\n","      <td>0.035286</td>\n","      <td>0.003772</td>\n","      <td>0.209700</td>\n","      <td>0.189606</td>\n","      <td>-0.059739</td>\n","      <td>-0.307844</td>\n","      <td>1.355347</td>\n","      <td>...</td>\n","      <td>-0.379288</td>\n","      <td>-0.846366</td>\n","      <td>1.199650</td>\n","      <td>-1.522898</td>\n","      <td>-0.122562</td>\n","      <td>-0.070323</td>\n","      <td>0.049260</td>\n","      <td>0.177152</td>\n","      <td>-0.256289</td>\n","      <td>-0.081080</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>OML LN Equity</td>\n","      <td>2011-12-26</td>\n","      <td>2012-12-24</td>\n","      <td>0.026060</td>\n","      <td>0.005241</td>\n","      <td>2.246997</td>\n","      <td>-0.144441</td>\n","      <td>0.277326</td>\n","      <td>1.226403</td>\n","      <td>1.460132</td>\n","      <td>...</td>\n","      <td>1.055692</td>\n","      <td>-1.223971</td>\n","      <td>0.045677</td>\n","      <td>-0.923465</td>\n","      <td>0.386810</td>\n","      <td>-0.766281</td>\n","      <td>0.520807</td>\n","      <td>0.419635</td>\n","      <td>-0.157164</td>\n","      <td>0.860373</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>OMI US Equity</td>\n","      <td>2011-12-26</td>\n","      <td>2012-12-24</td>\n","      <td>0.022801</td>\n","      <td>0.000982</td>\n","      <td>-0.559379</td>\n","      <td>-0.576399</td>\n","      <td>-0.359849</td>\n","      <td>0.541192</td>\n","      <td>2.323201</td>\n","      <td>...</td>\n","      <td>-0.821738</td>\n","      <td>0.401402</td>\n","      <td>0.169911</td>\n","      <td>0.032740</td>\n","      <td>-0.149242</td>\n","      <td>-0.119050</td>\n","      <td>-2.563318</td>\n","      <td>0.818454</td>\n","      <td>0.943932</td>\n","      <td>0.112350</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>144149</th>\n","      <td>EXEL US Equity</td>\n","      <td>2016-12-19</td>\n","      <td>2017-12-18</td>\n","      <td>0.064486</td>\n","      <td>0.009458</td>\n","      <td>0.105186</td>\n","      <td>-1.007860</td>\n","      <td>-0.658160</td>\n","      <td>2.407820</td>\n","      <td>0.652714</td>\n","      <td>...</td>\n","      <td>1.761518</td>\n","      <td>-1.276974</td>\n","      <td>-0.672783</td>\n","      <td>0.607669</td>\n","      <td>-0.918858</td>\n","      <td>-0.386630</td>\n","      <td>0.288501</td>\n","      <td>0.537541</td>\n","      <td>-0.807384</td>\n","      <td>-0.353482</td>\n","    </tr>\n","    <tr>\n","      <th>144150</th>\n","      <td>EXP US Equity</td>\n","      <td>2016-12-19</td>\n","      <td>2017-12-18</td>\n","      <td>0.024582</td>\n","      <td>0.001305</td>\n","      <td>2.050313</td>\n","      <td>-0.049513</td>\n","      <td>-0.423011</td>\n","      <td>0.144621</td>\n","      <td>0.679837</td>\n","      <td>...</td>\n","      <td>0.321916</td>\n","      <td>-0.270173</td>\n","      <td>-0.937015</td>\n","      <td>-0.941007</td>\n","      <td>0.189209</td>\n","      <td>0.384867</td>\n","      <td>-0.389854</td>\n","      <td>3.224025</td>\n","      <td>0.495431</td>\n","      <td>-1.438569</td>\n","    </tr>\n","    <tr>\n","      <th>144151</th>\n","      <td>EXPD US Equity</td>\n","      <td>2016-12-19</td>\n","      <td>2017-12-18</td>\n","      <td>0.019928</td>\n","      <td>0.001883</td>\n","      <td>4.982290</td>\n","      <td>-0.753607</td>\n","      <td>-0.790787</td>\n","      <td>-0.018738</td>\n","      <td>0.684680</td>\n","      <td>...</td>\n","      <td>-1.040213</td>\n","      <td>0.177536</td>\n","      <td>-0.622907</td>\n","      <td>-1.028403</td>\n","      <td>1.553974</td>\n","      <td>-0.365500</td>\n","      <td>0.881032</td>\n","      <td>3.983572</td>\n","      <td>-0.693140</td>\n","      <td>0.527320</td>\n","    </tr>\n","    <tr>\n","      <th>144152</th>\n","      <td>EWT US Equity</td>\n","      <td>2016-12-19</td>\n","      <td>2017-12-18</td>\n","      <td>0.016259</td>\n","      <td>0.005000</td>\n","      <td>-1.994484</td>\n","      <td>-1.345934</td>\n","      <td>0.133875</td>\n","      <td>1.569252</td>\n","      <td>0.719807</td>\n","      <td>...</td>\n","      <td>1.724402</td>\n","      <td>-0.764204</td>\n","      <td>0.230405</td>\n","      <td>-0.161596</td>\n","      <td>-1.024008</td>\n","      <td>-0.062274</td>\n","      <td>-0.421819</td>\n","      <td>-1.445724</td>\n","      <td>-0.993960</td>\n","      <td>-0.055474</td>\n","    </tr>\n","    <tr>\n","      <th>144153</th>\n","      <td>ZURN SW Equity</td>\n","      <td>2016-12-19</td>\n","      <td>2017-12-18</td>\n","      <td>0.014366</td>\n","      <td>0.002684</td>\n","      <td>1.037904</td>\n","      <td>0.384283</td>\n","      <td>-0.459373</td>\n","      <td>0.848136</td>\n","      <td>-0.949220</td>\n","      <td>...</td>\n","      <td>-0.352465</td>\n","      <td>0.567276</td>\n","      <td>0.975307</td>\n","      <td>-0.163771</td>\n","      <td>-0.649152</td>\n","      <td>-0.535586</td>\n","      <td>-1.480893</td>\n","      <td>0.920521</td>\n","      <td>-0.514812</td>\n","      <td>1.552666</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>144154 rows Ã— 58 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8fc46a3-ab51-4e0f-a915-140a4911d16f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b8fc46a3-ab51-4e0f-a915-140a4911d16f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b8fc46a3-ab51-4e0f-a915-140a4911d16f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4b2b3aa8-39d6-4250-b5bf-fcfbf94ca8a8\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4b2b3aa8-39d6-4250-b5bf-fcfbf94ca8a8')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4b2b3aa8-39d6-4250-b5bf-fcfbf94ca8a8 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                    Ticker       Start         End       Std      Mean  \\\n","0       0848680D US Equity  2011-12-26  2012-12-24  0.070169  0.018523   \n","1             ON US Equity  2011-12-26  2012-12-24  0.045480 -0.004158   \n","2            OMV AV Equity  2011-12-26  2012-12-24  0.035286  0.003772   \n","3            OML LN Equity  2011-12-26  2012-12-24  0.026060  0.005241   \n","4            OMI US Equity  2011-12-26  2012-12-24  0.022801  0.000982   \n","...                    ...         ...         ...       ...       ...   \n","144149      EXEL US Equity  2016-12-19  2017-12-18  0.064486  0.009458   \n","144150       EXP US Equity  2016-12-19  2017-12-18  0.024582  0.001305   \n","144151      EXPD US Equity  2016-12-19  2017-12-18  0.019928  0.001883   \n","144152       EWT US Equity  2016-12-19  2017-12-18  0.016259  0.005000   \n","144153      ZURN SW Equity  2016-12-19  2017-12-18  0.014366  0.002684   \n","\n","          Target         0         1         2         3  ...        42  \\\n","0      -0.571562 -0.080081 -1.143180  0.431351 -0.639025  ... -0.289046   \n","1       2.034040  0.435876  1.908284 -0.641805  2.036218  ...  0.164235   \n","2       0.209700  0.189606 -0.059739 -0.307844  1.355347  ... -0.379288   \n","3       2.246997 -0.144441  0.277326  1.226403  1.460132  ...  1.055692   \n","4      -0.559379 -0.576399 -0.359849  0.541192  2.323201  ... -0.821738   \n","...          ...       ...       ...       ...       ...  ...       ...   \n","144149  0.105186 -1.007860 -0.658160  2.407820  0.652714  ...  1.761518   \n","144150  2.050313 -0.049513 -0.423011  0.144621  0.679837  ...  0.321916   \n","144151  4.982290 -0.753607 -0.790787 -0.018738  0.684680  ... -1.040213   \n","144152 -1.994484 -1.345934  0.133875  1.569252  0.719807  ...  1.724402   \n","144153  1.037904  0.384283 -0.459373  0.848136 -0.949220  ... -0.352465   \n","\n","              43        44        45        46        47        48        49  \\\n","0      -0.748845  0.395460 -0.615254 -0.417208 -0.085343 -0.012613 -0.414256   \n","1       0.451889  1.172715 -0.811848 -0.887582  1.158916  1.444163  0.584085   \n","2      -0.846366  1.199650 -1.522898 -0.122562 -0.070323  0.049260  0.177152   \n","3      -1.223971  0.045677 -0.923465  0.386810 -0.766281  0.520807  0.419635   \n","4       0.401402  0.169911  0.032740 -0.149242 -0.119050 -2.563318  0.818454   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","144149 -1.276974 -0.672783  0.607669 -0.918858 -0.386630  0.288501  0.537541   \n","144150 -0.270173 -0.937015 -0.941007  0.189209  0.384867 -0.389854  3.224025   \n","144151  0.177536 -0.622907 -1.028403  1.553974 -0.365500  0.881032  3.983572   \n","144152 -0.764204  0.230405 -0.161596 -1.024008 -0.062274 -0.421819 -1.445724   \n","144153  0.567276  0.975307 -0.163771 -0.649152 -0.535586 -1.480893  0.920521   \n","\n","              50        51  \n","0      -0.518012 -0.418591  \n","1       0.763100 -0.483041  \n","2      -0.256289 -0.081080  \n","3      -0.157164  0.860373  \n","4       0.943932  0.112350  \n","...          ...       ...  \n","144149 -0.807384 -0.353482  \n","144150  0.495431 -1.438569  \n","144151 -0.693140  0.527320  \n","144152 -0.993960 -0.055474  \n","144153 -0.514812  1.552666  \n","\n","[144154 rows x 58 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Get data\n","data = pd.read_csv(f'{path}/Data/Train_data.csv')\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":368,"status":"ok","timestamp":1702925788641,"user":{"displayName":"Jacob Eriksen","userId":"07841893324515475290"},"user_tz":-60},"id":"BwwYrfZ-iCnJ","outputId":"03a3d7c5-b9a4-435b-a5d3-16cc64d9b821"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of training sequences: 122536\n","Number of validation sequences: 21618\n"]}],"source":["\n","# Creating the train and validation data\n","\n","np.random.seed(43)\n","# Stratified sample based on the end date\n","validation_df = data.groupby('End', group_keys=False).apply(lambda x: x.sample(frac=0.15))\n","#validation_df = data.sample(frac=0.15)\n","train_df = data[ ~data.index.isin(validation_df.index)]\n","\n","print(f'Number of training sequences: {len(train_df)}')\n","print(f'Number of validation sequences: {len(validation_df)}')"]},{"cell_type":"markdown","metadata":{"id":"5ZbsYLeziCnJ"},"source":["### Training the model\n","\n","This section creates training loops for the models\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5z1e2Re6oPKp"},"outputs":[],"source":["######### MSE Model #########\n","\n","model_version = 'MSE_model'\n","\n","# Define the loss function\n","criterion = nn.MSELoss().cuda()\n","corr = utils.NegCorr().cuda()\n","\n","batch_size = 128\n","enc_seq_len = 48 # length of input given to encoder\n","output_sequence_length = 1 # target sequence length. The length we want to predict\n","\n","## Creates the mask (Same for ecery iteration in the training since it's just a mask)\n","# Make src mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, enc_seq_len]\n","src_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=enc_seq_len\n","    ).cuda()\n","\n","# Make tgt mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n","tgt_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=output_sequence_length\n","    ).cuda()\n","\n","# Define the model\n","model = tsModel.TimeSeriesTransformer(\n","                                      input_size=1,\n","                                      dec_seq_len=enc_seq_len,\n","                                      batch_first=False,\n","                                      num_predicted_features=1,\n","                                      n_encoder_layers = n_encoder_layers,\n","                                      n_decoder_layers = n_decoder_layers,\n","                                      n_heads = n_heads,\n","                                      PE = 'original',\n","                                      batch_size = batch_size\n","                                      ).cuda()\n","\n","\n","## Creating the train\n","\n","## Creating the train\n","\n","# Chooses the optimizer and set learning rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n","\n","# If we want to train the model from a certain already trained epoch\n","#model.load_state_dict(torch.load(f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_20.pth'))\n","\n","# Number of epochs\n","epochs = 100\n","\n","# List for our losses\n","losses = []\n","v_losses = []\n","corr_loss = []\n","\n","loss_min = 100\n","\n","# Training loop\n","for i in range(epochs):\n","\n","    # Creates a decreasing learning rate based on the current epoch\n","    if i > 100:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000005)\n","    if i > 125:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n","    if i > 150:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.0000005)\n","    if i > 150:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.0000001)\n","\n","    # Makes sure we train on the data\n","    model.train(True)\n","\n","    # Create data\n","    input_data = utils.dataset(train_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect train losses for the average loss of the training set\n","    t_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in tqdm(input_data):\n","\n","        # Change the dimensions\n","        src, trg, trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","        # Send data through the model and get the prediction results\n","        y_pred = model.forward( src=src,\n","                                tgt=trg,\n","                                src_mask=src_mask,\n","                                tgt_mask=tgt_mask\n","                                )\n","\n","        # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","        loss = criterion(y_pred.squeeze(-1), trg_y)\n","\n","\n","        # #Sending the train data through the model\n","        # y_pred = utils.run_encoder_decoder_inference(\n","        #                                             model=model,\n","        #                                             src=src,\n","        #                                             forecast_window=output_sequence_length,\n","        #                                             batch_size=src.shape[1],\n","        #                                             device = device\n","        #                                             )\n","\n","        # # Stacking mean and std for the loss function\n","        # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","        # std = torch.vstack([std, std, std, std]).to(device)\n","\n","        #Calculating the loss/error on the test data\n","        # loss = criterion(y_pred.squeeze(-1), trg_y)\n","\n","        # Saves the vloss\n","        t_loss.append(loss.cpu().detach().numpy())\n","\n","        # The backwards propagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        #print(f'Batch loss: {loss}')\n","\n","    # Averaging over the t_loss of every batch\n","    avg_tloss = np.asarray(t_loss).mean()\n","\n","    # Saves the loss\n","    losses.append(avg_tloss)\n","\n","    # Turns off the training, since we want to calculate the validation loss\n","    model.train(False)\n","\n","    # Create validation data\n","    validation_data = utils.dataset(validation_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect validation losses for the average loss of the validation\n","    v_loss = []\n","    c_loss = []\n","\n","    # Loop over each batch\n","    for v_src, v_trg, v_trg_y, mean, std in validation_data:\n","\n","      # Change the dimensions\n","      v_src, v_trg, v_trg_y = utils.change_dim(v_src, v_trg, v_trg_y, batch_first)\n","\n","      # Send data through the model and get the prediction results\n","      y_vpred = model.forward( src=v_src,\n","                              tgt=v_trg,\n","                              src_mask=src_mask,\n","                              tgt_mask=tgt_mask\n","                              )\n","\n","      # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","      vloss = criterion(y_vpred.squeeze(-1), v_trg_y)\n","\n","\n","      # # Sending the test data through the model\n","      # y_vpred = utils.run_encoder_decoder_inference(\n","      #                                               model=model,\n","      #                                               src=v_src,\n","      #                                               forecast_window=output_sequence_length,\n","      #                                               batch_size=src.shape[1],\n","      #                                               device = device\n","      #                                               )\n","\n","      # # Stacking mean and std for the loss function\n","      # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","      # std = torch.vstack([std, std, std, std]).to(device)\n","\n","      # # Calculating the loss/error on the validation data\n","      # vloss = criterion.loss(y_vpred.squeeze(-1), v_trg_y, mean, std)\n","\n","      closs = corr.loss(y_vpred.squeeze(-1).squeeze(-1), v_trg_y.squeeze(-1))\n","\n","      # Saves the vloss\n","      v_loss.append(vloss.cpu().detach().numpy())\n","\n","      c_loss.append(closs.cpu().detach().numpy())\n","\n","    # Averaging over the v_loss of every batch\n","    avg_vloss = np.asarray(v_loss).mean()\n","\n","    avg_closs = np.asarray(c_loss).mean()\n","\n","    # Saves average v_loss for the epoch\n","    v_losses.append(avg_vloss)\n","\n","    corr_loss.append(avg_closs)\n","\n","    # Saves the model parameters whenever the average validation loss is the lowest occured\n","    if avg_vloss < loss_min:\n","      loss_min = avg_vloss\n","      torch.save(model.state_dict(), f'{path}/Models/{model_version}/Model_parameters.pth')\n","\n","    # Saves all the individual parameters for every epoch\n","    torch.save(model.state_dict(), f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_{i}.pth')\n","\n","    # Print the losses during the loop\n","    print(f'Epoch {i} with loss: {avg_tloss}, validation loss: {avg_vloss} and Negative Correlation: {avg_closs}')\n","\n","\n","# Plots the train and validation loss values\n","plt.plot( losses[:])\n","plt.plot( v_losses[:])\n","plt.ylabel('MSE')\n","plt.xlabel('Epoch')\n","\n","loss = pd.DataFrame([losses, v_losses, corr_loss], index= ['Train loss', 'Validation loss', 'Negative correlation']).T\n","loss.to_excel(f'{path}/Models/{model_version}/Training loss values.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYimfJLLMytZ"},"outputs":[],"source":["######### MSE_T2V Model #########\n","\n","model_version = 'MSE_T2V'\n","\n","# Define the loss function\n","criterion = nn.MSELoss().cuda()\n","corr = utils.NegCorr().cuda()\n","\n","batch_size = 1024\n","enc_seq_len = 48 # length of input given to encoder\n","output_sequence_length = 1 # target sequence length. The length we want to predict\n","\n","## Creates the mask (Same for ecery iteration in the training since it's just a mask)\n","# Make src mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, enc_seq_len]\n","src_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=enc_seq_len\n","    ).cuda()\n","\n","# Make tgt mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n","tgt_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=output_sequence_length\n","    ).cuda()\n","\n","# Define the model\n","model = tsModel.TimeSeriesTransformer(\n","                                      input_size=1,\n","                                      dec_seq_len=enc_seq_len,\n","                                      batch_first=False,\n","                                      num_predicted_features=1,\n","                                      n_encoder_layers = n_encoder_layers,\n","                                      n_decoder_layers = n_decoder_layers,\n","                                      n_heads = n_heads,\n","                                      PE = 'T2V',\n","                                      batch_size = batch_size\n","                                      ).cuda()\n","\n","\n","## Creating the train\n","\n","# Chooses the optimizer and set learning rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","\n","# If we want to train the model from a certain already trained epoch\n","#model.load_state_dict(torch.load(f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_20.pth'))\n","\n","# Number of epochs\n","epochs = 75\n","\n","# List for our losses\n","losses = []\n","v_losses = []\n","corr_loss = []\n","\n","loss_min = 100\n","\n","# Training loop\n","for i in range(epochs):\n","\n","    # Creates a decreasing learning rate based on the current epoch\n","    if i > 25:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n","    if i > 50:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n","    if i > 75:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000005)\n","    if i > 125:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n","\n","    # Makes sure we train on the data\n","    model.train(True)\n","\n","    # Create data\n","    input_data = utils.dataset(train_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect train losses for the average loss of the training set\n","    t_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in tqdm(input_data):\n","\n","        # Change the dimensions\n","        src, trg, trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","        # Send data through the model and get the prediction results\n","        y_pred = model.forward( src=src,\n","                                tgt=trg,\n","                                src_mask=src_mask,\n","                                tgt_mask=tgt_mask\n","                                )\n","\n","        # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","        loss = criterion(y_pred.squeeze(-1), trg_y)\n","\n","\n","        # #Sending the train data through the model\n","        # y_pred = utils.run_encoder_decoder_inference(\n","        #                                             model=model,\n","        #                                             src=src,\n","        #                                             forecast_window=output_sequence_length,\n","        #                                             batch_size=src.shape[1],\n","        #                                             device = device\n","        #                                             )\n","\n","        # # Stacking mean and std for the loss function\n","        # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","        # std = torch.vstack([std, std, std, std]).to(device)\n","\n","        #Calculating the loss/error on the test data\n","        # loss = criterion(y_pred.squeeze(-1), trg_y)\n","\n","        # Saves the vloss\n","        t_loss.append(loss.cpu().detach().numpy())\n","\n","        # The backwards propagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        #print(f'Batch loss: {loss}')\n","\n","    # Averaging over the t_loss of every batch\n","    avg_tloss = np.asarray(t_loss).mean()\n","\n","    # Saves the loss\n","    losses.append(avg_tloss)\n","\n","    # Turns off the training, since we want to calculate the validation loss\n","    model.train(False)\n","\n","    # Create validation data\n","    validation_data = utils.dataset(validation_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect validation losses for the average loss of the validation\n","    v_loss = []\n","    c_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in validation_data:\n","\n","      # Change the dimensions\n","      v_src, v_trg, v_trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","      # Send data through the model and get the prediction results\n","      y_vpred = model.forward( src=v_src,\n","                              tgt=v_trg,\n","                              src_mask=src_mask,\n","                              tgt_mask=tgt_mask\n","                              )\n","\n","      # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","      vloss = criterion(y_vpred.squeeze(-1), v_trg_y)\n","\n","\n","      # # Sending the test data through the model\n","      # y_vpred = utils.run_encoder_decoder_inference(\n","      #                                               model=model,\n","      #                                               src=v_src,\n","      #                                               forecast_window=output_sequence_length,\n","      #                                               batch_size=src.shape[1],\n","      #                                               device = device\n","      #                                               )\n","\n","      # # Stacking mean and std for the loss function\n","      # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","      # std = torch.vstack([std, std, std, std]).to(device)\n","\n","      # # Calculating the loss/error on the validation data\n","      # vloss = criterion.loss(y_vpred.squeeze(-1), v_trg_y, mean, std)\n","\n","      closs = corr.loss(y_vpred.squeeze(-1), v_trg_y)\n","\n","      # Saves the vloss\n","      v_loss.append(vloss.cpu().detach().numpy())\n","\n","      c_loss.append(closs.cpu().detach().numpy())\n","\n","    # Averaging over the v_loss of every batch\n","    avg_vloss = np.asarray(v_loss).mean()\n","\n","    avg_closs = np.asarray(c_loss).mean()\n","\n","    # Saves average v_loss for the epoch\n","    v_losses.append(avg_vloss)\n","\n","    corr_loss.append(avg_closs)\n","\n","    # Saves the model parameters whenever the average validation loss is the lowest occured\n","    if avg_vloss < loss_min:\n","      loss_min = avg_vloss\n","      torch.save(model.state_dict(), f'{path}/Models/{model_version}/Model_parameters.pth')\n","\n","    # Saves all the individual parameters for every epoch\n","    torch.save(model.state_dict(), f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_{i}.pth')\n","\n","    # Print the losses during the loop\n","    print(f'Epoch {i} with loss: {avg_tloss}, validation loss: {avg_vloss} and Negative Correlation: {avg_closs}')\n","\n","\n","# Plots the train and validation loss values\n","plt.plot( losses[:])\n","plt.plot( v_losses[:])\n","plt.ylabel('MSE')\n","plt.xlabel('Epoch')\n","\n","loss = pd.DataFrame([losses, v_losses, corr_loss], index= ['Train loss', 'Validation loss', 'Negative correlation']).T\n","loss.to_excel(f'{path}/Models/{model_version}/Training loss values.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_v9igSZJMpyN"},"outputs":[],"source":["######### WMSE Model #########\n","\n","model_version = 'WMSE_model'\n","\n","# Define the loss function\n","criterion = utils.WMSE().cuda()\n","corr = utils.NegCorr().cuda()\n","\n","batch_size = 128\n","enc_seq_len = 48 # length of input given to encoder\n","output_sequence_length = 1 # target sequence length. The length we want to predict\n","\n","## Creates the mask (Same for ecery iteration in the training since it's just a mask)\n","# Make src mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, enc_seq_len]\n","src_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=enc_seq_len\n","    ).cuda()\n","\n","# Make tgt mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n","tgt_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=output_sequence_length\n","    ).cuda()\n","\n","# Define the model\n","model = tsModel.TimeSeriesTransformer(\n","                                      input_size=1,\n","                                      dec_seq_len=enc_seq_len,\n","                                      batch_first=False,\n","                                      num_predicted_features=1,\n","                                      n_encoder_layers = n_encoder_layers,\n","                                      n_decoder_layers = n_decoder_layers,\n","                                      n_heads = n_heads,\n","                                      PE = 'original',\n","                                      batch_size = batch_size\n","                                      ).cuda()\n","\n","\n","## Creating the train\n","\n","# Chooses the optimizer and set learning rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","\n","# If we want to train the model from a certain already trained epoch\n","#model.load_state_dict(torch.load(f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_20.pth'))\n","\n","# Number of epochs\n","epochs = 100\n","\n","# List for our losses\n","losses = []\n","v_losses = []\n","corr_loss = []\n","\n","loss_min = 100\n","\n","# Training loop\n","for i in range(epochs):\n","\n","    # Creates a decreasing learning rate based on the current epoch\n","    if i > 25:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n","    if i > 50:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n","    if i > 75:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000005)\n","    if i > 125:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n","\n","    # Makes sure we train on the data\n","    model.train(True)\n","\n","    # Create data\n","    input_data = utils.dataset(train_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect train losses for the average loss of the training set\n","    t_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in tqdm(input_data):\n","\n","        # Change the dimensions\n","        src, trg, trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","        # Send data through the model and get the prediction results\n","        y_pred = model.forward( src=src,\n","                                tgt=trg,\n","                                src_mask=src_mask,\n","                                tgt_mask=tgt_mask\n","                                )\n","\n","        # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","        loss = criterion.loss(y_pred.squeeze(-1), trg_y)\n","\n","\n","        # #Sending the train data through the model\n","        # y_pred = utils.run_encoder_decoder_inference(\n","        #                                             model=model,\n","        #                                             src=src,\n","        #                                             forecast_window=output_sequence_length,\n","        #                                             batch_size=src.shape[1],\n","        #                                             device = device\n","        #                                             )\n","\n","        # # Stacking mean and std for the loss function\n","        # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","        # std = torch.vstack([std, std, std, std]).to(device)\n","\n","        #Calculating the loss/error on the test data\n","        # loss = criterion(y_pred.squeeze(-1), trg_y)\n","\n","        # Saves the vloss\n","        t_loss.append(loss.cpu().detach().numpy())\n","\n","        # The backwards propagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        #print(f'Batch loss: {loss}')\n","\n","    # Averaging over the t_loss of every batch\n","    avg_tloss = np.asarray(t_loss).mean()\n","\n","    # Saves the loss\n","    losses.append(avg_tloss)\n","\n","    # Turns off the training, since we want to calculate the validation loss\n","    model.train(False)\n","\n","    # Create validation data\n","    validation_data = utils.dataset(validation_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect validation losses for the average loss of the validation\n","    v_loss = []\n","    c_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in validation_data:\n","\n","      # Change the dimensions\n","      v_src, v_trg, v_trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","      # Send data through the model and get the prediction results\n","      y_vpred = model.forward( src=v_src,\n","                              tgt=v_trg,\n","                              src_mask=src_mask,\n","                              tgt_mask=tgt_mask\n","                              )\n","\n","      # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","      vloss = criterion.loss(y_vpred.squeeze(-1), v_trg_y)\n","\n","\n","      # # Sending the test data through the model\n","      # y_vpred = utils.run_encoder_decoder_inference(\n","      #                                               model=model,\n","      #                                               src=v_src,\n","      #                                               forecast_window=output_sequence_length,\n","      #                                               batch_size=src.shape[1],\n","      #                                               device = device\n","      #                                               )\n","\n","      # # Stacking mean and std for the loss function\n","      # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","      # std = torch.vstack([std, std, std, std]).to(device)\n","\n","      # # Calculating the loss/error on the validation data\n","      # vloss = criterion.loss(y_vpred.squeeze(-1), v_trg_y, mean, std)\n","\n","      closs = corr.loss(y_vpred.squeeze(-1), v_trg_y)\n","\n","      # Saves the vloss\n","      v_loss.append(vloss.cpu().detach().numpy())\n","\n","      c_loss.append(closs.cpu().detach().numpy())\n","\n","    # Averaging over the v_loss of every batch\n","    avg_vloss = np.asarray(v_loss).mean()\n","\n","    avg_closs = np.asarray(c_loss).mean()\n","\n","    # Saves average v_loss for the epoch\n","    v_losses.append(avg_vloss)\n","\n","    corr_loss.append(avg_closs)\n","\n","    # Saves the model parameters whenever the average validation loss is the lowest occured\n","    if avg_vloss < loss_min:\n","      loss_min = avg_vloss\n","      torch.save(model.state_dict(), f'{path}/Models/{model_version}/Model_parameters.pth')\n","\n","    # Saves all the individual parameters for every epoch\n","    torch.save(model.state_dict(), f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_{i}.pth')\n","\n","    # Print the losses during the loop\n","    print(f'Epoch {i} with loss: {avg_tloss}, validation loss: {avg_vloss} and Negative Correlation: {avg_closs}')\n","\n","\n","# Plots the train and validation loss values\n","plt.plot( losses[:])\n","plt.plot( v_losses[:])\n","plt.ylabel('MSE')\n","plt.xlabel('Epoch')\n","\n","loss = pd.DataFrame([losses, v_losses, corr_loss], index= ['Train loss', 'Validation loss', 'Negative correlation']).T\n","loss.to_excel(f'{path}/Models/{model_version}/Training loss values.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7j1bVctMie2"},"outputs":[],"source":["######### AdjMSE Model #########\n","\n","model_version = 'AdjMSE_model'\n","\n","# Define the loss function\n","criterion = utils.AdjMSELoss2().cuda()\n","corr = utils.NegCorr().cuda()\n","\n","batch_size = 128\n","enc_seq_len = 48 # length of input given to encoder\n","output_sequence_length = 1 # target sequence length. The length we want to predict\n","\n","## Creates the mask (Same for ecery iteration in the training since it's just a mask)\n","# Make src mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, enc_seq_len]\n","src_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=enc_seq_len\n","    ).cuda()\n","\n","# Make tgt mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n","tgt_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=output_sequence_length\n","    ).cuda()\n","\n","# Define the model\n","model = tsModel.TimeSeriesTransformer(\n","                                      input_size=1,\n","                                      dec_seq_len=enc_seq_len,\n","                                      batch_first=False,\n","                                      num_predicted_features=1,\n","                                      n_encoder_layers = n_encoder_layers,\n","                                      n_decoder_layers = n_decoder_layers,\n","                                      n_heads = n_heads,\n","                                      PE = 'original',\n","                                      batch_size = batch_size\n","                                      ).cuda()\n","\n","\n","## Creating the train\n","\n","# Chooses the optimizer and set learning rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","\n","# If we want to train the model from a certain already trained epoch\n","#model.load_state_dict(torch.load(f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_20.pth'))\n","\n","# Number of epochs\n","epochs = 100\n","\n","# List for our losses\n","losses = []\n","v_losses = []\n","corr_loss = []\n","\n","loss_min = 100\n","\n","# Training loop\n","for i in range(epochs):\n","\n","    # Creates a decreasing learning rate based on the current epoch\n","    if i > 25:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n","    if i > 50:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n","    if i > 75:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000005)\n","    if i > 125:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n","\n","    # Makes sure we train on the data\n","    model.train(True)\n","\n","    # Create data\n","    input_data = utils.dataset(train_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect train losses for the average loss of the training set\n","    t_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in tqdm(input_data):\n","\n","        # Change the dimensions\n","        src, trg, trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","        # Send data through the model and get the prediction results\n","        y_pred = model.forward( src=src,\n","                                tgt=trg,\n","                                src_mask=src_mask,\n","                                tgt_mask=tgt_mask\n","                                )\n","\n","        # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","        loss = criterion.loss(y_pred.squeeze(-1), trg_y)\n","\n","\n","        # #Sending the train data through the model\n","        # y_pred = utils.run_encoder_decoder_inference(\n","        #                                             model=model,\n","        #                                             src=src,\n","        #                                             forecast_window=output_sequence_length,\n","        #                                             batch_size=src.shape[1],\n","        #                                             device = device\n","        #                                             )\n","\n","        # # Stacking mean and std for the loss function\n","        # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","        # std = torch.vstack([std, std, std, std]).to(device)\n","\n","        #Calculating the loss/error on the test data\n","        # loss = criterion(y_pred.squeeze(-1), trg_y)\n","\n","        # Saves the vloss\n","        t_loss.append(loss.cpu().detach().numpy())\n","\n","        # The backwards propagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        #print(f'Batch loss: {loss}')\n","\n","    # Averaging over the t_loss of every batch\n","    avg_tloss = np.asarray(t_loss).mean()\n","\n","    # Saves the loss\n","    losses.append(avg_tloss)\n","\n","    # Turns off the training, since we want to calculate the validation loss\n","    model.train(False)\n","\n","    # Create validation data\n","    validation_data = utils.dataset(validation_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect validation losses for the average loss of the validation\n","    v_loss = []\n","    c_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in validation_data:\n","\n","      # Change the dimensions\n","      v_src, v_trg, v_trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","      # Send data through the model and get the prediction results\n","      y_vpred = model.forward( src=v_src,\n","                              tgt=v_trg,\n","                              src_mask=src_mask,\n","                              tgt_mask=tgt_mask\n","                              )\n","\n","      # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","      vloss = criterion.loss(y_vpred.squeeze(-1), v_trg_y)\n","\n","\n","      # # Sending the test data through the model\n","      # y_vpred = utils.run_encoder_decoder_inference(\n","      #                                               model=model,\n","      #                                               src=v_src,\n","      #                                               forecast_window=output_sequence_length,\n","      #                                               batch_size=src.shape[1],\n","      #                                               device = device\n","      #                                               )\n","\n","      # # Stacking mean and std for the loss function\n","      # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","      # std = torch.vstack([std, std, std, std]).to(device)\n","\n","      # # Calculating the loss/error on the validation data\n","      # vloss = criterion.loss(y_vpred.squeeze(-1), v_trg_y, mean, std)\n","\n","      closs = corr.loss(y_vpred.squeeze(-1), v_trg_y)\n","\n","      # Saves the vloss\n","      v_loss.append(vloss.cpu().detach().numpy())\n","\n","      c_loss.append(closs.cpu().detach().numpy())\n","\n","    # Averaging over the v_loss of every batch\n","    avg_vloss = np.asarray(v_loss).mean()\n","\n","    avg_closs = np.asarray(c_loss).mean()\n","\n","    # Saves average v_loss for the epoch\n","    v_losses.append(avg_vloss)\n","\n","    corr_loss.append(avg_closs)\n","\n","    # Saves the model parameters whenever the average validation loss is the lowest occured\n","    if avg_vloss < loss_min:\n","      loss_min = avg_vloss\n","      torch.save(model.state_dict(), f'{path}/Models/{model_version}/Model_parameters.pth')\n","\n","    # Saves all the individual parameters for every epoch\n","    torch.save(model.state_dict(), f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_{i}.pth')\n","\n","    # Print the losses during the loop\n","    print(f'Epoch {i} with loss: {avg_tloss}, validation loss: {avg_vloss} and Negative Correlation: {avg_closs}')\n","\n","\n","# Plots the train and validation loss values\n","plt.plot( losses[:])\n","plt.plot( v_losses[:])\n","plt.ylabel('MSE')\n","plt.xlabel('Epoch')\n","\n","loss = pd.DataFrame([losses, v_losses, corr_loss], index= ['Train loss', 'Validation loss', 'Negative correlation']).T\n","loss.to_excel(f'{path}/Models/{model_version}/Training loss values.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPd4j2zGMbuK"},"outputs":[],"source":["######### NegCorr Model #########\n","\n","model_version = 'NegCorr_model'\n","\n","# Define the loss function\n","criterion = utils.NegCorr().cuda()\n","corr = nn.MSELoss().cuda()\n","\n","batch_size = 128\n","enc_seq_len = 48 # length of input given to encoder\n","output_sequence_length = 1 # target sequence length. The length we want to predict\n","\n","## Creates the mask (Same for ecery iteration in the training since it's just a mask)\n","# Make src mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, enc_seq_len]\n","src_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=enc_seq_len\n","    ).cuda()\n","\n","# Make tgt mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n","tgt_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=output_sequence_length\n","    ).cuda()\n","\n","# Define the model\n","model = tsModel.TimeSeriesTransformer(\n","                                      input_size=1,\n","                                      dec_seq_len=enc_seq_len,\n","                                      batch_first=False,\n","                                      num_predicted_features=1,\n","                                      n_encoder_layers = n_encoder_layers,\n","                                      n_decoder_layers = n_decoder_layers,\n","                                      n_heads = n_heads,\n","                                      PE = 'original',\n","                                      batch_size = batch_size\n","                                      ).cuda()\n","\n","\n","## Creating the train\n","\n","# Chooses the optimizer and set learning rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n","\n","# If we want to train the model from a certain already trained epoch\n","#model.load_state_dict(torch.load(f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_20.pth'))\n","\n","# Number of epochs\n","epochs = 150\n","\n","# List for our losses\n","losses = []\n","v_losses = []\n","corr_loss = []\n","\n","loss_min = 100\n","\n","# Training loop\n","for i in range(epochs):\n","\n","    # Creates a decreasing learning rate based on the current epoch\n","    if i > 50:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000005)\n","    if i > 100:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n","    if i > 150:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.0000005)\n","    if i > 200:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.0000001)\n","\n","    # Makes sure we train on the data\n","    model.train(True)\n","\n","    # Create data\n","    input_data = utils.dataset(train_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect train losses for the average loss of the training set\n","    t_loss = []\n","\n","    # Creates a list of all unique dates in the training data\n","    train_dates = train_df['End'].unique()\n","    # Shuffles the list for training\n","    random.shuffle(train_dates)\n","\n","\n","    for d in tqdm(train_dates):\n","\n","      # Create data\n","      input_data = utils.dataset(train_df[ train_df['End'] == d], enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","      # Loop over each batch\n","      for src, trg, trg_y, mean, std in input_data:\n","\n","          # Change the dimensions\n","          src, trg, trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","          # Send data through the model and get the prediction results\n","          y_pred = model.forward( src=src,\n","                                  tgt=trg,\n","                                  src_mask=src_mask,\n","                                  tgt_mask=tgt_mask\n","                                  )\n","\n","          # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","          loss = criterion.loss(y_pred.squeeze(-1), trg_y)\n","\n","\n","          # #Sending the train data through the model\n","          # y_pred = utils.run_encoder_decoder_inference(\n","          #                                             model=model,\n","          #                                             src=src,\n","          #                                             forecast_window=output_sequence_length,\n","          #                                             batch_size=src.shape[1],\n","          #                                             device = device\n","          #                                             )\n","\n","          # # Stacking mean and std for the loss function\n","          # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","          # std = torch.vstack([std, std, std, std]).to(device)\n","\n","          #Calculating the loss/error on the test data\n","          # loss = criterion(y_pred.squeeze(-1), trg_y)\n","\n","          # Saves the vloss\n","          t_loss.append(loss.cpu().detach().numpy())\n","\n","          # The backwards propagation\n","          optimizer.zero_grad()\n","          loss.backward()\n","          optimizer.step()\n","\n","          #print(f'Batch loss: {loss}')\n","\n","    # Averaging over the t_loss of every batch\n","    avg_tloss = np.asarray(t_loss).mean()\n","\n","    # Saves the loss\n","    losses.append(avg_tloss)\n","\n","    # Turns off the training, since we want to calculate the validation loss\n","    model.train(False)\n","\n","    # Create validation data\n","    validation_data = utils.dataset(validation_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect validation losses for the average loss of the validation\n","    v_loss = []\n","    c_loss = []\n","\n","    # Creates a list of all unique dates in the validation data\n","    validation_dates = validation_df['End'].unique()\n","\n","    for d in validation_dates:\n","\n","      # Create validation data\n","      validation_data = utils.dataset(validation_df[ validation_df['End'] == d], enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= len(validation_df[ validation_df['End'] == d]) , device= device)\n","\n","      # Loop over each batch\n","      for src, trg, trg_y, mean, std in validation_data:\n","\n","        # Change the dimensions\n","        v_src, v_trg, v_trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","        # Send data through the model and get the prediction results\n","        y_vpred = model.forward( src=v_src,\n","                                tgt=v_trg,\n","                                src_mask=src_mask,\n","                                tgt_mask=tgt_mask\n","                                )\n","\n","        # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","        vloss = criterion.loss(y_vpred.squeeze(-1), v_trg_y)\n","\n","\n","        # # Sending the test data through the model\n","        # y_vpred = utils.run_encoder_decoder_inference(\n","        #                                               model=model,\n","        #                                               src=v_src,\n","        #                                               forecast_window=output_sequence_length,\n","        #                                               batch_size=src.shape[1],\n","        #                                               device = device\n","        #                                               )\n","\n","        # # Stacking mean and std for the loss function\n","        # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","        # std = torch.vstack([std, std, std, std]).to(device)\n","\n","        # # Calculating the loss/error on the validation data\n","        # vloss = criterion.loss(y_vpred.squeeze(-1), v_trg_y, mean, std)\n","\n","        # Standardize both validation predictions and target before calculating MSE\n","        y_vpred = ((y_vpred.squeeze(-1) - torch.mean(y_vpred.squeeze(-1))) /torch.std(y_vpred.squeeze(-1)))\n","        v_trg_y = ((v_trg_y - torch.mean(v_trg_y)) /torch.std(v_trg_y))\n","\n","        closs = corr(y_vpred, v_trg_y)\n","\n","        # Saves the vloss\n","        v_loss.append(vloss.cpu().detach().numpy())\n","\n","        c_loss.append(closs.cpu().detach().numpy())\n","\n","    # Averaging over the v_loss of every batch\n","    avg_vloss = np.asarray(v_loss).mean()\n","\n","    avg_closs = np.asarray(c_loss).mean()\n","\n","    # Saves average v_loss for the epoch\n","    v_losses.append(avg_vloss)\n","\n","    corr_loss.append(avg_closs)\n","\n","    # Saves the model parameters whenever the average validation loss is the lowest occured\n","    if avg_vloss < loss_min:\n","      loss_min = avg_vloss\n","      torch.save(model.state_dict(), f'{path}/Models/{model_version}/Model_parameters.pth')\n","\n","    # Saves all the individual parameters for every epoch\n","    torch.save(model.state_dict(), f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_{i}.pth')\n","\n","    # Print the losses during the loop\n","    print(f'Epoch {i} with loss: {avg_tloss}, validation loss: {avg_vloss} and MSE: {avg_closs}')\n","\n","\n","# Plots the train and validation loss values\n","plt.plot( losses[:])\n","plt.plot( v_losses[:])\n","plt.ylabel('MSE')\n","plt.xlabel('Epoch')\n","\n","loss = pd.DataFrame([losses, v_losses, corr_loss], index= ['Train loss', 'Validation loss', 'MSE']).T\n","loss.to_excel(f'{path}/Models/{model_version}/Training loss values.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SDXZWmPtMUtm"},"outputs":[],"source":["######### LSTM Model #########\n","\n","model_version = 'LSTM_model'\n","\n","# Define the loss function\n","criterion = nn.MSELoss().cuda()\n","corr = utils.NegCorr().cuda()\n","\n","batch_size = 128\n","\n","input_size = 1\n","seq_len = 48\n","output_size = 1\n","hidden_size = 100\n","\n","# Define the LSTM model\n","class LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(LSTM, self).__init__()\n","        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n","        self.fc = torch.nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n","        return out\n","\n","model = LSTM(input_size, hidden_size, output_size).cuda()\n","\n","\n","## Creating the train\n","\n","# Chooses the optimizer and set learning rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","\n","# If we want to train the model from a certain already trained epoch\n","#model.load_state_dict(torch.load(f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_20.pth'))\n","\n","# Number of epochs\n","epochs = 100\n","\n","# List for our losses\n","losses = []\n","v_losses = []\n","corr_loss = []\n","\n","loss_min = 100\n","\n","# Training loop\n","for i in range(epochs):\n","\n","    # Creates a decreasing learning rate based on the current epoch\n","    if i > 50:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n","    if i > 75:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n","    if i > 100:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000005)\n","    if i > 125:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n","\n","    # Makes sure we train on the data\n","    model.train(True)\n","\n","    # Create data\n","    input_data = utils.dataset(train_df, enc_seq_len=seq_len, trg_seq_len= output_size, batch_size= batch_size, device= device)\n","\n","    # List to collect train losses for the average loss of the training set\n","    t_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in tqdm(input_data):\n","\n","        # Change the dimensions\n","        src, trg, trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","        # Transpose the data to match the LSTM model\n","        trg_y = torch.transpose(trg_y, 0, 1)\n","        src = torch.transpose(src, 0, 1)\n","\n","        # Send data through the model and get the prediction results\n","        y_pred = model.forward( src  )\n","\n","        # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","        loss = criterion(y_pred.squeeze(-1), trg_y.squeeze(-1))\n","\n","\n","        # #Sending the train data through the model\n","        # y_pred = utils.run_encoder_decoder_inference(\n","        #                                             model=model,\n","        #                                             src=src,\n","        #                                             forecast_window=output_sequence_length,\n","        #                                             batch_size=src.shape[1],\n","        #                                             device = device\n","        #                                             )\n","\n","        # # Stacking mean and std for the loss function\n","        # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","        # std = torch.vstack([std, std, std, std]).to(device)\n","\n","        #Calculating the loss/error on the test data\n","        # loss = criterion(y_pred.squeeze(-1), trg_y)\n","\n","        # Saves the vloss\n","        t_loss.append(loss.cpu().detach().numpy())\n","\n","        # The backwards propagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        #print(f'Batch loss: {loss}')\n","\n","    # Averaging over the t_loss of every batch\n","    avg_tloss = np.asarray(t_loss).mean()\n","\n","    # Saves the loss\n","    losses.append(avg_tloss)\n","\n","    # Turns off the training, since we want to calculate the validation loss\n","    model.train(False)\n","\n","    # Create validation data\n","    validation_data = utils.dataset(validation_df, enc_seq_len=seq_len, trg_seq_len= output_size, batch_size= batch_size, device= device)\n","\n","    # List to collect validation losses for the average loss of the validation\n","    v_loss = []\n","    c_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in validation_data:\n","\n","      # Change the dimensions\n","      v_src, v_trg, v_trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","      # Transpose the data to match the LSTM model\n","      v_trg_y = torch.transpose(v_trg_y, 0, 1)\n","      v_src = torch.transpose(v_src, 0, 1)\n","\n","      # Send data through the model and get the prediction results\n","      y_vpred = model.forward( src  )\n","\n","      # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","      vloss = criterion(y_vpred.squeeze(-1), v_trg_y.squeeze(-1))\n","\n","\n","      # # Sending the test data through the model\n","      # y_vpred = utils.run_encoder_decoder_inference(\n","      #                                               model=model,\n","      #                                               src=v_src,\n","      #                                               forecast_window=output_sequence_length,\n","      #                                               batch_size=src.shape[1],\n","      #                                               device = device\n","      #                                               )\n","\n","      # # Stacking mean and std for the loss function\n","      # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","      # std = torch.vstack([std, std, std, std]).to(device)\n","\n","      # # Calculating the loss/error on the validation data\n","      # vloss = criterion.loss(y_vpred.squeeze(-1), v_trg_y, mean, std)\n","\n","      closs = corr.loss(y_vpred.squeeze(-1), v_trg_y.squeeze(-1))\n","\n","      # Saves the vloss\n","      v_loss.append(vloss.cpu().detach().numpy())\n","\n","      c_loss.append(closs.cpu().detach().numpy())\n","\n","    # Averaging over the v_loss of every batch\n","    avg_vloss = np.asarray(v_loss).mean()\n","\n","    avg_closs = np.asarray(c_loss).mean()\n","\n","    # Saves average v_loss for the epoch\n","    v_losses.append(avg_vloss)\n","\n","    corr_loss.append(avg_closs)\n","\n","    # Saves the model parameters whenever the average validation loss is the lowest occured\n","    if avg_vloss < loss_min:\n","      loss_min = avg_vloss\n","      torch.save(model.state_dict(), f'{path}/Models/{model_version}/Model_parameters.pth')\n","\n","    # Saves all the individual parameters for every epoch\n","    torch.save(model.state_dict(), f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_{i}.pth')\n","\n","    # Print the losses during the loop\n","    print(f'Epoch {i} with loss: {avg_tloss}, validation loss: {avg_vloss} and Negative Correlation: {avg_closs}')\n","\n","\n","# Plots the train and validation loss values\n","plt.plot( losses[:])\n","plt.plot( v_losses[:])\n","plt.ylabel('MSE')\n","plt.xlabel('Epoch')\n","\n","loss = pd.DataFrame([losses, v_losses, corr_loss], index= ['Train loss', 'Validation loss', 'Negative correlation']).T\n","loss.to_excel(f'{path}/Models/{model_version}/Training loss values.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnbKIYG_vzkA"},"outputs":[],"source":["######### MSE Model #########\n","\n","model_version = 'MSE_single'\n","\n","# Define the loss function\n","criterion = nn.MSELoss().cuda()\n","corr = utils.NegCorr().cuda()\n","\n","batch_size = 128\n","enc_seq_len = 48 # length of input given to encoder\n","output_sequence_length = 4 # target sequence length. The length we want to predict\n","\n","## Creates the mask (Same for ecery iteration in the training since it's just a mask)\n","# Make src mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, enc_seq_len]\n","src_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=enc_seq_len\n","    ).cuda()\n","\n","# Make tgt mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n","tgt_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=output_sequence_length\n","    ).cuda()\n","\n","# Define the model\n","model = tsModel.TimeSeriesTransformer(\n","                                      input_size=1,\n","                                      dec_seq_len=enc_seq_len,\n","                                      batch_first=False,\n","                                      num_predicted_features=1,\n","                                      n_encoder_layers = n_encoder_layers,\n","                                      n_decoder_layers = n_decoder_layers,\n","                                      n_heads = n_heads,\n","                                      PE = 'original',\n","                                      batch_size = batch_size\n","                                      ).cuda()\n","\n","## Creating the train\n","\n","# Chooses the optimizer and set learning rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","\n","# If we want to train the model from a certain already trained epoch\n","#model.load_state_dict(torch.load(f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_20.pth'))\n","\n","# Number of epochs\n","epochs = 100\n","\n","# List for our losses\n","losses = []\n","v_losses = []\n","corr_loss = []\n","\n","loss_min = 100\n","\n","# Training loop\n","for i in range(epochs):\n","\n","    # Creates a decreasing learning rate based on the current epoch\n","    if i > 25:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n","    if i > 50:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n","    if i > 75:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000005)\n","    if i > 125:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n","\n","    # Makes sure we train on the data\n","    model.train(True)\n","\n","    # Create data\n","    input_data = utils.dataset(train_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect train losses for the average loss of the training set\n","    t_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in tqdm(input_data):\n","\n","        # Change the dimensions\n","        src, trg, trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","        # Send data through the model and get the prediction results\n","        y_pred = model.forward( src=src,\n","                                tgt=trg,\n","                                src_mask=src_mask,\n","                                tgt_mask=tgt_mask\n","                                )\n","\n","        # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","        loss = criterion(y_pred.squeeze(-1), trg_y)\n","\n","\n","        # #Sending the train data through the model\n","        # y_pred = utils.run_encoder_decoder_inference(\n","        #                                             model=model,\n","        #                                             src=src,\n","        #                                             forecast_window=output_sequence_length,\n","        #                                             batch_size=src.shape[1],\n","        #                                             device = device\n","        #                                             )\n","\n","        # # Stacking mean and std for the loss function\n","        # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","        # std = torch.vstack([std, std, std, std]).to(device)\n","\n","        # #Calculating the loss/error on the test data\n","        # loss = criterion.loss(y_pred.squeeze(-1), trg_y, mean, std)\n","\n","        # Saves the vloss\n","        t_loss.append(loss.cpu().detach().numpy())\n","\n","        # The backwards propagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        #print(f'Batch loss: {loss}')\n","\n","    # Averaging over the t_loss of every batch\n","    avg_tloss = np.asarray(t_loss).mean()\n","\n","    # Saves the loss\n","    losses.append(avg_tloss)\n","\n","    # Turns off the training, since we want to calculate the validation loss\n","    model.train(False)\n","\n","    # Create validation data\n","    validation_data = utils.dataset(validation_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect validation losses for the average loss of the validation\n","    v_loss = []\n","    c_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in validation_data:\n","\n","      # Change the dimensions\n","      v_src, v_trg, v_trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","      # Send data through the model and get the prediction results\n","      y_vpred = model.forward(  src=v_src,\n","                                tgt=v_trg,\n","                                src_mask=src_mask,\n","                                tgt_mask=tgt_mask\n","                                )\n","\n","      # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","      vloss = criterion(y_vpred.squeeze(-1), v_trg_y)\n","\n","\n","      # # Sending the test data through the model\n","      # y_vpred = utils.run_encoder_decoder_inference(\n","      #                                               model=model,\n","      #                                               src=v_src,\n","      #                                               forecast_window=output_sequence_length,\n","      #                                               batch_size=src.shape[1],\n","      #                                               device = device\n","      #                                               )\n","\n","      # Stacking mean and std for the loss function\n","      mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","      std = torch.vstack([std, std, std, std]).to(device)\n","\n","      # # Calculating the loss/error on the validation data\n","      # vloss = criterion.loss(y_vpred.squeeze(-1), v_trg_y, mean, std)\n","\n","      # Reverting the scaling so we can sum across the 4 weeks so we can calculate correlation\n","      y_vpred_corr = (y_vpred.squeeze(-1) * std) + mean\n","      v_trg_y_corr = (v_trg_y * std) + mean\n","\n","      # Calculate correlation loss\n","      closs = corr.loss(y_vpred_corr.sum(axis=0), v_trg_y.sum(axis=0))\n","\n","      # Saves the vloss\n","      v_loss.append(vloss.cpu().detach().numpy())\n","\n","      c_loss.append(closs.cpu().detach().numpy())\n","\n","    # Averaging over the v_loss of every batch\n","    avg_vloss = np.asarray(v_loss).mean()\n","\n","    avg_closs = np.asarray(c_loss).mean()\n","\n","    # Saves average v_loss for the epoch\n","    v_losses.append(avg_vloss)\n","\n","    corr_loss.append(avg_closs)\n","\n","    # Saves the model parameters whenever the average validation loss is the lowest occured\n","    if avg_vloss < loss_min:\n","      loss_min = avg_vloss\n","      torch.save(model.state_dict(), f'{path}/Models/{model_version}/Model_parameters.pth')\n","\n","    # Saves all the individual parameters for every epoch\n","    torch.save(model.state_dict(), f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_{i}.pth')\n","\n","    # Print the losses during the loop\n","    print(f'Epoch {i} with loss: {avg_tloss}, validation loss: {avg_vloss} and Negative Correlation: {avg_closs}')\n","\n","\n","# Plots the train and validation loss values\n","plt.plot( losses[:])\n","plt.plot( v_losses[:])\n","plt.ylabel('MSE')\n","plt.xlabel('Epoch')\n","\n","loss = pd.DataFrame([losses, v_losses, corr_loss], index= ['Train loss', 'Validation loss', 'Negative correlation']).T\n","loss.to_excel(f'{path}/Models/{model_version}/Training loss values.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqUyTv9qMJ9S"},"outputs":[],"source":["######### LSTM Model #########\n","\n","model_version = 'LSTM_single'\n","\n","# Define the loss function\n","criterion = nn.MSELoss().cuda()\n","corr = utils.NegCorr().cuda()\n","\n","batch_size = 128\n","\n","input_size = 1\n","seq_len = 48\n","output_size = 4\n","hidden_size = 100\n","\n","# Define the LSTM model\n","class LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(LSTM, self).__init__()\n","        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n","        self.fc = torch.nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n","        return out\n","\n","model = LSTM(input_size, hidden_size, output_size).cuda()\n","\n","\n","## Creating the train\n","\n","# Chooses the optimizer and set learning rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","\n","# If we want to train the model from a certain already trained epoch\n","#model.load_state_dict(torch.load(f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_20.pth'))\n","\n","# Number of epochs\n","epochs = 150\n","\n","# List for our losses\n","losses = []\n","v_losses = []\n","corr_loss = []\n","\n","loss_min = 100\n","\n","# Training loop\n","for i in range(epochs):\n","\n","    # Creates a decreasing learning rate based on the current epoch\n","    if i > 50:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n","    if i > 75:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n","    if i > 100:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000005)\n","    if i > 125:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n","\n","    # Makes sure we train on the data\n","    model.train(True)\n","\n","    # Create data\n","    input_data = utils.dataset(train_df, enc_seq_len=seq_len, trg_seq_len= output_size, batch_size= batch_size, device= device)\n","\n","    # List to collect train losses for the average loss of the training set\n","    t_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in tqdm(input_data):\n","\n","        # Change the dimensions\n","        src, trg, trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","        # Transpose the data to match the LSTM model\n","        trg_y = torch.transpose(trg_y, 0, 1)\n","        src = torch.transpose(src, 0, 1)\n","\n","        # Send data through the model and get the prediction results\n","        y_pred = model.forward( src  )\n","\n","        # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","        loss = criterion(y_pred.squeeze(-1), trg_y.squeeze(-1))\n","\n","\n","        # #Sending the train data through the model\n","        # y_pred = utils.run_encoder_decoder_inference(\n","        #                                             model=model,\n","        #                                             src=src,\n","        #                                             forecast_window=output_sequence_length,\n","        #                                             batch_size=src.shape[1],\n","        #                                             device = device\n","        #                                             )\n","\n","        # # Stacking mean and std for the loss function\n","        # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","        # std = torch.vstack([std, std, std, std]).to(device)\n","\n","        #Calculating the loss/error on the test data\n","        # loss = criterion(y_pred.squeeze(-1), trg_y)\n","\n","        # Saves the vloss\n","        t_loss.append(loss.cpu().detach().numpy())\n","\n","        # The backwards propagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        #print(f'Batch loss: {loss}')\n","\n","    # Averaging over the t_loss of every batch\n","    avg_tloss = np.asarray(t_loss).mean()\n","\n","    # Saves the loss\n","    losses.append(avg_tloss)\n","\n","    # Turns off the training, since we want to calculate the validation loss\n","    model.train(False)\n","\n","    # Create validation data\n","    validation_data = utils.dataset(validation_df, enc_seq_len=seq_len, trg_seq_len= output_size, batch_size= batch_size, device= device)\n","\n","    # List to collect validation losses for the average loss of the validation\n","    v_loss = []\n","    c_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in validation_data:\n","\n","      # Change the dimensions\n","      v_src, v_trg, v_trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","      # Transpose the data to match the LSTM model\n","      v_trg_y = torch.transpose(v_trg_y, 0, 1)\n","      v_src = torch.transpose(v_src, 0, 1)\n","\n","      # Send data through the model and get the prediction results\n","      y_vpred = model.forward( src  )\n","\n","      # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","      vloss = criterion(y_vpred.squeeze(-1), v_trg_y.squeeze(-1))\n","\n","\n","      # # Sending the test data through the model\n","      # y_vpred = utils.run_encoder_decoder_inference(\n","      #                                               model=model,\n","      #                                               src=v_src,\n","      #                                               forecast_window=output_sequence_length,\n","      #                                               batch_size=src.shape[1],\n","      #                                               device = device\n","      #                                               )\n","\n","      # # Stacking mean and std for the loss function\n","      # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","      # std = torch.vstack([std, std, std, std]).to(device)\n","\n","      # # Calculating the loss/error on the validation data\n","      # vloss = criterion.loss(y_vpred.squeeze(-1), v_trg_y, mean, std)\n","\n","      closs = corr.loss(y_vpred.squeeze(-1), v_trg_y.squeeze(-1))\n","\n","      # Saves the vloss\n","      v_loss.append(vloss.cpu().detach().numpy())\n","\n","      c_loss.append(closs.cpu().detach().numpy())\n","\n","    # Averaging over the v_loss of every batch\n","    avg_vloss = np.asarray(v_loss).mean()\n","\n","    avg_closs = np.asarray(c_loss).mean()\n","\n","    # Saves average v_loss for the epoch\n","    v_losses.append(avg_vloss)\n","\n","    corr_loss.append(avg_closs)\n","\n","    # Saves the model parameters whenever the average validation loss is the lowest occured\n","    if avg_vloss < loss_min:\n","      loss_min = avg_vloss\n","      torch.save(model.state_dict(), f'{path}/Models/{model_version}/Model_parameters.pth')\n","\n","    # Saves all the individual parameters for every epoch\n","    torch.save(model.state_dict(), f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_{i}.pth')\n","\n","    # Print the losses during the loop\n","    print(f'Epoch {i} with loss: {avg_tloss}, validation loss: {avg_vloss} and Negative Correlation: {avg_closs}')\n","\n","\n","# Plots the train and validation loss values\n","plt.plot( losses[:])\n","plt.plot( v_losses[:])\n","plt.ylabel('MSE')\n","plt.xlabel('Epoch')\n","\n","loss = pd.DataFrame([losses, v_losses, corr_loss], index= ['Train loss', 'Validation loss', 'Negative correlation']).T\n","loss.to_excel(f'{path}/Models/{model_version}/Training loss values.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jDyK84inYYW"},"outputs":[],"source":["######### MSE Model #########\n","\n","model_version = 'MSE_test'\n","\n","# Define the loss function\n","criterion = nn.MSELoss().cuda()\n","corr = utils.NegCorr().cuda()\n","\n","batch_size = 128\n","enc_seq_len = 48 # length of input given to encoder\n","output_sequence_length = 1 # target sequence length. The length we want to predict\n","\n","## Creates the mask (Same for ecery iteration in the training since it's just a mask)\n","# Make src mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, enc_seq_len]\n","src_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=enc_seq_len\n","    ).cuda()\n","\n","# Make tgt mask for decoder with size:\n","# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n","tgt_mask = utils.generate_square_subsequent_mask(\n","    dim1=output_sequence_length,\n","    dim2=output_sequence_length\n","    ).cuda()\n","\n","# Define the model\n","model = tsModel.TimeSeriesTransformer(\n","                                      input_size=1,\n","                                      dec_seq_len=enc_seq_len,\n","                                      batch_first=False,\n","                                      num_predicted_features=1,\n","                                      n_encoder_layers = n_encoder_layers,\n","                                      n_decoder_layers = n_decoder_layers,\n","                                      n_heads = n_heads,\n","                                      PE = 'original',\n","                                      batch_size = batch_size\n","                                      ).cuda()\n","\n","\n","## Creating the train\n","\n","# Chooses the optimizer and set learning rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n","\n","# If we want to train the model from a certain already trained epoch\n","#model.load_state_dict(torch.load(f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_20.pth'))\n","\n","# Number of epochs\n","epochs = 150\n","\n","# List for our losses\n","losses = []\n","v_losses = []\n","corr_loss = []\n","\n","loss_min = 100\n","\n","# Training loop\n","for i in range(epochs):\n","\n","    # Creates a decreasing learning rate based on the current epoch\n","    if i > 50:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000005)\n","    if i > 100:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n","    if i > 125:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.0000005)\n","    if i > 150:\n","      optimizer = torch.optim.Adam(model.parameters(), lr=0.0000001)\n","\n","    # Makes sure we train on the data\n","    model.train(True)\n","\n","    # Create data\n","    input_data = utils.dataset(train_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect train losses for the average loss of the training set\n","    t_loss = []\n","\n","    # Loop over each batch\n","    for src, trg, trg_y, mean, std in tqdm(input_data):\n","\n","        # Change the dimensions\n","        src, trg, trg_y = utils.change_dim(src, trg, trg_y, batch_first)\n","\n","        # Send data through the model and get the prediction results\n","        y_pred = model.forward( src=src,\n","                                tgt=trg,\n","                                src_mask=src_mask,\n","                                tgt_mask=tgt_mask\n","                                )\n","\n","        # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","        loss = criterion(y_pred.squeeze(-1), trg_y)\n","\n","\n","        # #Sending the train data through the model\n","        # y_pred = utils.run_encoder_decoder_inference(\n","        #                                             model=model,\n","        #                                             src=src,\n","        #                                             forecast_window=output_sequence_length,\n","        #                                             batch_size=src.shape[1],\n","        #                                             device = device\n","        #                                             )\n","\n","        # # Stacking mean and std for the loss function\n","        # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","        # std = torch.vstack([std, std, std, std]).to(device)\n","\n","        #Calculating the loss/error on the test data\n","        # loss = criterion(y_pred.squeeze(-1), trg_y)\n","\n","        # Saves the vloss\n","        t_loss.append(loss.cpu().detach().numpy())\n","\n","        # The backwards propagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        #print(f'Batch loss: {loss}')\n","\n","    # Averaging over the t_loss of every batch\n","    avg_tloss = np.asarray(t_loss).mean()\n","\n","    # Saves the loss\n","    losses.append(avg_tloss)\n","\n","    # Turns off the training, since we want to calculate the validation loss\n","    model.train(False)\n","\n","    # Create validation data\n","    validation_data = utils.dataset(validation_df, enc_seq_len=enc_seq_len, trg_seq_len= output_sequence_length, batch_size= batch_size, device= device)\n","\n","    # List to collect validation losses for the average loss of the validation\n","    v_loss = []\n","    c_loss = []\n","\n","    # Loop over each batch\n","    for v_src, v_trg, v_trg_y, mean, std in validation_data:\n","\n","      # Change the dimensions\n","      v_src, v_trg, v_trg_y = utils.change_dim(v_src, v_trg, v_trg_y, batch_first)\n","\n","      # Send data through the model and get the prediction results\n","      y_vpred = model.forward( src=v_src,\n","                              tgt=v_trg,\n","                              src_mask=src_mask,\n","                              tgt_mask=tgt_mask\n","                              )\n","\n","      # Measure the loss/error of the predicted value vs the train_target (trg_y)\n","      vloss = criterion(y_vpred.squeeze(-1), v_trg_y)\n","\n","\n","      # # Sending the test data through the model\n","      # y_vpred = utils.run_encoder_decoder_inference(\n","      #                                               model=model,\n","      #                                               src=v_src,\n","      #                                               forecast_window=output_sequence_length,\n","      #                                               batch_size=src.shape[1],\n","      #                                               device = device\n","      #                                               )\n","\n","      # # Stacking mean and std for the loss function\n","      # mean = torch.vstack([mean, mean, mean, mean]).to(device)\n","      # std = torch.vstack([std, std, std, std]).to(device)\n","\n","      # # Calculating the loss/error on the validation data\n","      # vloss = criterion.loss(y_vpred.squeeze(-1), v_trg_y, mean, std)\n","\n","      closs = corr.loss(y_vpred.squeeze(-1).squeeze(-1), v_trg_y.squeeze(-1))\n","\n","      # Saves the vloss\n","      v_loss.append(vloss.cpu().detach().numpy())\n","\n","      c_loss.append(closs.cpu().detach().numpy())\n","\n","    # Averaging over the v_loss of every batch\n","    avg_vloss = np.asarray(v_loss).mean()\n","\n","    avg_closs = np.asarray(c_loss).mean()\n","\n","    # Saves average v_loss for the epoch\n","    v_losses.append(avg_vloss)\n","\n","    corr_loss.append(avg_closs)\n","\n","    # Saves the model parameters whenever the average validation loss is the lowest occured\n","    if avg_vloss < loss_min:\n","      loss_min = avg_vloss\n","      torch.save(model.state_dict(), f'{path}/Models/{model_version}/Model_parameters.pth')\n","\n","    # Saves all the individual parameters for every epoch\n","    torch.save(model.state_dict(), f'{path}/Models/{model_version}/Training parameters/Parameters_epoch_{i}.pth')\n","\n","    # Print the losses during the loop\n","    print(f'Epoch {i} with loss: {avg_tloss}, validation loss: {avg_vloss} and Negative Correlation: {avg_closs}')\n","\n","\n","# Plots the train and validation loss values\n","plt.plot( losses[:])\n","plt.plot( v_losses[:])\n","plt.ylabel('MSE')\n","plt.xlabel('Epoch')\n","\n","loss = pd.DataFrame([losses, v_losses, corr_loss], index= ['Train loss', 'Validation loss', 'Negative correlation']).T\n","loss.to_excel(f'{path}/Models/{model_version}/Training loss values.xlsx')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
